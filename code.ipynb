{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4767ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               id1      id2        id3  \\\n",
      "0  1366776_189706075_16-23_2023-11-02 22:22:00.042  1366776  189706075   \n",
      "1      1366776_89227_16-23_2023-11-01 23:51:24.999  1366776      89227   \n",
      "2      1366776_35046_16-23_2023-11-01 00:30:59.797  1366776      35046   \n",
      "3    1366776_6275451_16-23_2023-11-02 22:21:32.261  1366776    6275451   \n",
      "4      1366776_78053_16-23_2023-11-02 22:21:34.799  1366776      78053   \n",
      "\n",
      "                       id4         id5  y   f1    f2    f3    f4  ...  f357  \\\n",
      "0  2023-11-02 22:22:00.042  2023-11-02  0  1.0  None  None  None  ...  None   \n",
      "1  2023-11-01 23:51:24.999  2023-11-01  0  1.0  None  None  None  ...  None   \n",
      "2  2023-11-01 00:30:59.797  2023-11-01  0  1.0  None  None  None  ...  None   \n",
      "3  2023-11-02 22:21:32.261  2023-11-02  0  1.0  None  None  None  ...  None   \n",
      "4  2023-11-02 22:21:34.799  2023-11-02  0  1.0  None  None  None  ...  None   \n",
      "\n",
      "      f358 f359  f360   f361 f362               f363    f364 f365  \\\n",
      "0  -9999.0  0.0  None   28.0  0.0                0.0   337.0  0.0   \n",
      "1     None  0.0  None   87.0  0.0                0.0  1010.0  2.0   \n",
      "2     None  0.0  None   23.0  0.0                0.0  1010.0  2.0   \n",
      "3  -9999.0  0.0  None  277.0  1.0  0.003610108303249   337.0  0.0   \n",
      "4  -9999.0  0.0  None  359.0  0.0                0.0   337.0  0.0   \n",
      "\n",
      "                 f366  \n",
      "0                 0.0  \n",
      "1  0.0019801980198019  \n",
      "2  0.0019801980198019  \n",
      "3                 0.0  \n",
      "4                 0.0  \n",
      "\n",
      "[5 rows x 372 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Read parquet file\n",
    "table = pq.read_table(\"train_data (1).parquet\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_train = table.to_pandas()\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4811a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                     id1      id2     id3  \\\n",
      "46756    1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n",
      "57819       1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n",
      "15390   1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n",
      "145730     1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n",
      "146085      1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n",
      "...                                                 ...      ...     ...   \n",
      "1806        1874443_95537_16-23_2023-11-05 09:21:24.182  1874443   95537   \n",
      "127494       1541978_5718_16-23_2023-11-05 00:56:43.946  1541978    5718   \n",
      "106947      1887841_85905_16-23_2023-11-05 20:40:43.312  1887841   85905   \n",
      "158372     1569367_944713_16-23_2023-11-05 00:43:04.335  1569367  944713   \n",
      "74378       1086547_60142_16-23_2023-11-05 10:37:36.747  1086547   60142   \n",
      "\n",
      "                               id4         id5    f1    f2    f3    f4    f5  \\\n",
      "46756   2023-11-04 18:56:26.000794  2023-11-04  None  None  None  None  None   \n",
      "57819      2023-11-04 06:08:53.373  2023-11-04  None   9.0  None  None  None   \n",
      "15390   2023-11-05 10:07:28.000725  2023-11-05  None  None  None  None  22.0   \n",
      "145730     2023-11-04 12:25:28.244  2023-11-04  None  None  None  None  None   \n",
      "146085     2023-11-05 06:45:26.657  2023-11-05  None  None  None  None  None   \n",
      "...                            ...         ...   ...   ...   ...   ...   ...   \n",
      "1806       2023-11-05 09:21:24.182  2023-11-05  None  None  None  None  None   \n",
      "127494     2023-11-05 00:56:43.946  2023-11-05  23.0  None   1.0  None  10.0   \n",
      "106947     2023-11-05 20:40:43.312  2023-11-05  None  None  None  None  None   \n",
      "158372     2023-11-05 00:43:04.335  2023-11-05  None  None  None  None  None   \n",
      "74378      2023-11-05 10:37:36.747  2023-11-05  None  28.0  None  None  43.0   \n",
      "\n",
      "        ...                 f357                f358 f359  f360   f361  f362  \\\n",
      "46756   ...   0.0027807272790246  0.0465999838670646  0.0  None    1.0   0.0   \n",
      "57819   ...   0.0014287946777398  0.0603093514970909  0.0  None  195.0  13.0   \n",
      "15390   ...  -0.0174961119751166  0.0734836702954899  0.0  None  155.0  67.0   \n",
      "145730  ...   0.0013159476327096   0.040572039549215  0.0  None   None  None   \n",
      "146085  ...   0.0020541699799278   0.038243539079214  0.0  None   29.0   2.0   \n",
      "...     ...                  ...                 ...  ...   ...    ...   ...   \n",
      "1806    ...   0.0011444262884863  0.0451847712288842  0.0  None    5.0   0.0   \n",
      "127494  ...   0.0010418624094475  0.0552689756816507  0.0  None   18.0   0.0   \n",
      "106947  ...  -0.0158489476154373  0.0792447380771868  0.0  None   41.0   1.0   \n",
      "158372  ...   0.0019767025731701  0.0457251973961485  0.0  None  173.0  18.0   \n",
      "74378   ...  -0.0213071318171781  0.0894899536321483  0.0  None    1.0   0.0   \n",
      "\n",
      "                      f363    f364   f365                f366  \n",
      "46756                  0.0    56.0    0.0                 0.0  \n",
      "57819   0.0666666666666666    None   None                None  \n",
      "15390    0.432258064516129  1142.0  436.0  0.3817863397548161  \n",
      "145730                None    None   None                None  \n",
      "146085  0.0689655172413793   361.0    3.0  0.0083102493074792  \n",
      "...                    ...     ...    ...                 ...  \n",
      "1806                   0.0   144.0    8.0  0.0555555555555555  \n",
      "127494                 0.0   216.0    8.0   0.037037037037037  \n",
      "106947   0.024390243902439   569.0    1.0  0.0017574692442882  \n",
      "158372  0.1040462427745664    81.0   12.0  0.1481481481481481  \n",
      "74378                  0.0    None   None                None  \n",
      "\n",
      "[369301 rows x 371 columns]>\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_parquet(r\"C:\\Users\\gourh\\OneDrive\\Desktop\\esc\\AMEX\\test_data.parquet\")\n",
    "\n",
    "print(df_test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a32dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Columns dropped: 38\n",
      "🔹 Rows dropped from train: 4505\n",
      "📐 Shape overview:\n",
      " • Original train: (770164, 372)\n",
      " • Cleaned  train: (765659, 334)\n",
      " • Cleaned  test : (369301, 333)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ── PARAMETERS ──────────────────────────────────────────────\n",
    "TARGET          = \"y\"        # dependent variable\n",
    "NULL_LIMIT_COL  = 0.90       # drop column if ≥ 90 % nulls\n",
    "MIN_VARIETY_COL = 1          # drop column if ≤ 1 unique value\n",
    "NULL_LIMIT_ROW  = 0.50       # drop row if ≥ 50 % nulls (only for TRAIN)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "def clean_dataframes(df_train: pd.DataFrame,\n",
    "                     df_test: pd.DataFrame,\n",
    "                     target: str = TARGET,\n",
    "                     null_limit_col: float = NULL_LIMIT_COL,\n",
    "                     min_variety_col: int = MIN_VARIETY_COL,\n",
    "                     null_limit_row: float = NULL_LIMIT_ROW):\n",
    "    \"\"\"\n",
    "    Cleans train/test dataframes by:\n",
    "    1. Dropping constant or mostly-null columns (based on train only).\n",
    "    2. Dropping train rows with many nulls.\n",
    "    3. Aligning test columns (no row drop).\n",
    "    Returns: (df_train_final, df_test_final, dropped_cols, dropped_rows_count)\n",
    "    \"\"\"\n",
    "    # 1️⃣ Drop constant or mostly-null columns (from TRAIN only)\n",
    "    const_cols = [c for c in df_train.columns if df_train[c].nunique(dropna=False) <= min_variety_col]\n",
    "    null_cols  = [c for c in df_train.columns if df_train[c].isna().mean() >= null_limit_col]\n",
    "    to_drop = list({*const_cols, *null_cols} - {target})\n",
    "\n",
    "    # 2️⃣ Drop from both datasets\n",
    "    df_train_clean = df_train.drop(columns=to_drop, errors=\"ignore\")\n",
    "    df_test_clean  = df_test.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "    # 3️⃣ Drop TRAIN rows with many nulls (excluding target)\n",
    "    feature_cols = [c for c in df_train_clean.columns if c != target]\n",
    "    null_fraction_per_row = df_train_clean[feature_cols].isna().mean(axis=1)\n",
    "    bad_rows = null_fraction_per_row >= null_limit_row\n",
    "    df_train_final = df_train_clean.loc[~bad_rows].copy()\n",
    "    dropped_rows = bad_rows.sum()\n",
    "\n",
    "    # 4️⃣ Align TEST columns exactly with TRAIN features\n",
    "    df_test_final = df_test_clean.reindex(columns=feature_cols)\n",
    "\n",
    "    return df_train_final, df_test_final, to_drop, dropped_rows\n",
    "\n",
    "# ── RUN CLEANING ────────────────────────────────────────────\n",
    "df_train_final, df_test_final, dropped_cols, dropped_rows = clean_dataframes(df_train, df_test)\n",
    "\n",
    "# ── REPORT ──────────────────────────────────────────────────\n",
    "print(\"🔹 Columns dropped:\", len(dropped_cols))\n",
    "print(\"🔹 Rows dropped from train:\", dropped_rows)\n",
    "print(\"📐 Shape overview:\")\n",
    "print(\" • Original train:\", df_train.shape)\n",
    "print(\" • Cleaned  train:\", df_train_final.shape)\n",
    "print(\" • Cleaned  test :\", df_test_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88a3f81",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 3️⃣  XGBoost training ────────────────────────────────────\u001b[39;00m\n\u001b[32m     65\u001b[39m xgb_clf = XGBClassifier(\n\u001b[32m     66\u001b[39m     n_estimators=N_ESTIMATORS,\n\u001b[32m     67\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mlogloss\u001b[39m\u001b[33m\"\u001b[39m              \u001b[38;5;66;03m# ✅ moved here\u001b[39;00m\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mxgb_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEARLY_STOP_ROUNDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# print progress every 50 rounds\u001b[39;49;00m\n\u001b[32m     83\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_clf.best_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# 4️⃣  Validation metrics ──────────────────────────────────\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gourh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# ╭────────────────────────────────────────────────────────╮\n",
    "# │  FASTEST TABULAR PIPELINE (CPU “hist” or GPU “gpu_hist”)│\n",
    "# ╰────────────────────────────────────────────────────────╯\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# ── PARAMETERS ────────────────────────────────────────────\n",
    "TARGET          = \"y\"\n",
    "TEST_SIZE       = 0.20\n",
    "RANDOM_STATE    = 42\n",
    "TREE_METHOD     = \"hist\"    # ← change to \"gpu_hist\" if you have a CUDA GPU\n",
    "EARLY_STOP_ROUNDS = 25      # stop if no val‑improve for 25 rounds\n",
    "N_ESTIMATORS    = 1000      # large cap; early‑stopping will finish sooner\n",
    "# ──────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️⃣  Split         ───────────────────────────────────────\n",
    "X_df = df_train_final.drop(columns=TARGET)\n",
    "y_raw = df_train_final[TARGET]\n",
    "\n",
    "# label → int\n",
    "if y_raw.dtype.kind in {\"i\", \"u\"}:\n",
    "    y_all = y_raw.values\n",
    "else:\n",
    "    try:\n",
    "        y_all = y_raw.astype(int).values\n",
    "    except ValueError:\n",
    "        le = LabelEncoder()\n",
    "        y_all = le.fit_transform(y_raw)\n",
    "\n",
    "X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "    X_df, y_all,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "# 2️⃣  Preprocess once ─────────────────────────────────────\n",
    "cat_cols = [c for c in X_df.columns if X_df[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_df.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\",  StandardScaler())\n",
    "    ]), num_cols),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_cols)\n",
    "], sparse_threshold=0.3)           # ensure sparse output for speed\n",
    "\n",
    "X_train = preprocess.fit_transform(X_train_df)\n",
    "X_val   = preprocess.transform(X_val_df)\n",
    "X_test  = preprocess.transform(df_test_final[X_df.columns])\n",
    "\n",
    "# 3️⃣  XGBoost training ────────────────────────────────────\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=TREE_METHOD,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    reg_lambda=1.0,\n",
    "    eval_metric=\"logloss\"              # ✅ moved here\n",
    ")\n",
    "\n",
    "xgb_clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=EARLY_STOP_ROUNDS,\n",
    "    verbose=50         # print progress every 50 rounds\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {xgb_clf.best_iteration}\")\n",
    "\n",
    "# 4️⃣  Validation metrics ──────────────────────────────────\n",
    "y_pred_val = xgb_clf.predict(X_val)\n",
    "print(\"\\nValidation accuracy:\", f\"{accuracy_score(y_val, y_pred_val):.4%}\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# 5️⃣  Retrain on full data at best_iteration ──────────────\n",
    "best_n = xgb_clf.best_iteration or N_ESTIMATORS\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=best_n,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=TREE_METHOD,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    reg_lambda=1.0\n",
    ")\n",
    "# Reuse already‑fitted preprocess → avoid refitting OneHot\n",
    "X_all = sp.vstack([X_train, X_val])\n",
    "y_all = np.concatenate([y_train, y_val])\n",
    "xgb_final.fit(X_all, y_all)\n",
    "\n",
    "# 6️⃣  Predict test ────────────────────────────────────────\n",
    "test_pred = xgb_final.predict(X_test)\n",
    "if 'le' in locals():\n",
    "    test_pred = le.inverse_transform(test_pred)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"row_id\": df_test_final.index,   # replace with actual ID if present\n",
    "    \"y_pred\": test_pred\n",
    "})\n",
    "pred_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "print(\"\\n✅  Saved test_predictions.csv with\", len(pred_df), \"rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
